name: CreateTraningJob
description: Create the training job into Vertex training and launch it
inputs:
- {name: training_data, type: String, description: '(str) GCS path to the training
    dataset Tfrecords,'}
- {name: validation_data, type: String, description: '(str) GCS path to the validation
    dataset Tfrecords,'}
- {name: project, type: String, description: '(str) Name of the Google cloud project,'}
- {name: location, type: String, description: '(str) Training location in vetex (europe-west1,...)
    ,'}
- {name: bucket, type: String, description: '(str) GCS bucket to store data during
    the training,'}
- {name: batch_size, type: String, description: '(str) Training Batch size ''50'',',
  default: '50', optional: true}
- {name: validation_batch_size, type: String, description: '(str) Validation batch
    size defaut ''20'',', default: '20', optional: true}
- {name: training_ds_size, type: String, description: '(str) training dataset size
    default 25000,', default: '25000', optional: true}
- {name: validation_ds_size, type: String, description: '(str) validation dataset
    size default 5000,', default: '5000', optional: true}
- {name: img_height, type: String, description: '(str) image height size default 255,',
  default: '255', optional: true}
- {name: img_width, type: String, description: '(str) image widht size default 255,',
  default: '255', optional: true}
- {name: nb_classes, type: String, description: '(str) number of class default ''5'',',
  default: '5', optional: true}
- {name: display_name, type: String, description: (str)Vertex job display name default'quickdraw_training',
  default: quickdraw_training, optional: true}
outputs:
- {name: Output, type: String}
implementation:
  container:
    image: gcr.io/deeplearning-platform-release/tf2-cpu.2-8
    command:
    - sh
    - -ec
    - |
      program_path=$(mktemp)
      printf "%s" "$0" > "$program_path"
      python3 -u "$program_path" "$@"
    - "def createTraningJob(training_data,\n                     validation_data,\n\
      \                     project,\n                     location,\n           \
      \          bucket,\n                     batch_size = '50',\n              \
      \       validation_batch_size = '20',\n                     training_ds_size\
      \ = '25000',\n                     validation_ds_size = '5000',\n          \
      \           img_height = '255',\n                     img_width = '255',\n \
      \                    nb_classes = '5',\n                     display_name =\
      \ 'quickdraw_training'\n                    ):\n    \"\"\"\n    Create the training\
      \ job into Vertex training and launch it \n    :param training_data:  (str)\
      \ GCS path to the training dataset Tfrecords,\n    :param validation_data: (str)\
      \ GCS path to the validation dataset Tfrecords,\n    :param project: (str) Name\
      \ of the Google cloud project,\n    :param location: (str) Training location\
      \ in vetex (europe-west1,...) ,\n    :param bucket: (str) GCS bucket to store\
      \ data during the training,\n    :param batch_size: (str) Training Batch size\
      \ '50',\n    :param validation_batch_size: (str) Validation batch size defaut\
      \ '20',\n    :param  training_ds_size: (str) training dataset size default 25000,\n\
      \    :param  validation_ds_size: (str) validation dataset size default 5000,\n\
      \    :param  img_height: (str) image height size default 255,\n    :param  img_width:\
      \ (str) image widht size default 255,\n    :param  nb_classes: (str) number\
      \ of class default '5',\n    :param  display_name:(str)Vertex job display name\
      \ default'quickdraw_training'\n    :return: GCS path for the trained model\n\
      \    \"\"\"\n\n    from datetime import datetime \n    import google.cloud.aiplatform\
      \ as aip\n\n    display_job_name = display_name\n    staging_bucket = bucket+\"\
      staging/\"+display_job_name\n    model_path = bucket+\"gcs_model_data/\"+display_job_name\n\
      \n    env_var = {'GCS_TRAINING_DATA': training_data,\n               'GCS_VALIDATION_DATA':validation_data,\n\
      \                'GCS_MODEL_DATA_PATH': model_path\n                }\n\n  \
      \  #flo-test-devoxx/gcs_model_data/quickdraw_training_20220414_071940/model\n\
      \n    job = aip.CustomPythonPackageTrainingJob(\n        display_name=display_job_name,\n\
      \        python_package_gcs_uri= 'gs://devoxx_poc/vertex_job_code/quickdraw_classifier-0.0.1.tar.gz',\n\
      \        python_module_name=\"quickdraw_classifier.training\",\n        container_uri='europe-docker.pkg.dev/vertex-ai/training/tf-gpu.2-8:latest',\n\
      \        model_serving_container_image_uri='europe-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-8:latest',\n\
      \        project=project,\n        location=location,\n        staging_bucket=bucket\n\
      \    )\n\n    CMDARGS = [\n        '--batch_size='+batch_size,\n        '--validation_batch_size='+validation_batch_size,\n\
      \        '--training_ds_size='+training_ds_size,\n        '--validation_ds_size='+validation_ds_size,\n\
      \        '--img_height='+img_height,\n        '--img_width='+img_width,\n  \
      \      '--nb_classes='+nb_classes\n    ]\n\n    print(CMDARGS)\n\n    model\
      \ = job.run(\n        args=CMDARGS,\n        environment_variables=env_var,\n\
      \        sync=True,\n        replica_count=1,\n        machine_type='n1-standard-8',\n\
      \        accelerator_type='NVIDIA_TESLA_K80',\n        accelerator_count=1,\n\
      \        base_output_dir=bucket)\n\n    return model.name\n\ndef _serialize_str(str_value:\
      \ str) -> str:\n    if not isinstance(str_value, str):\n        raise TypeError('Value\
      \ \"{}\" has type \"{}\" instead of str.'.format(\n            str(str_value),\
      \ str(type(str_value))))\n    return str_value\n\nimport argparse\n_parser =\
      \ argparse.ArgumentParser(prog='CreateTraningJob', description='Create the training\
      \ job into Vertex training and launch it')\n_parser.add_argument(\"--training-data\"\
      , dest=\"training_data\", type=str, required=True, default=argparse.SUPPRESS)\n\
      _parser.add_argument(\"--validation-data\", dest=\"validation_data\", type=str,\
      \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--project\"\
      , dest=\"project\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
      --location\", dest=\"location\", type=str, required=True, default=argparse.SUPPRESS)\n\
      _parser.add_argument(\"--bucket\", dest=\"bucket\", type=str, required=True,\
      \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--batch-size\", dest=\"\
      batch_size\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
      --validation-batch-size\", dest=\"validation_batch_size\", type=str, required=False,\
      \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--training-ds-size\", dest=\"\
      training_ds_size\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
      --validation-ds-size\", dest=\"validation_ds_size\", type=str, required=False,\
      \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--img-height\", dest=\"\
      img_height\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
      --img-width\", dest=\"img_width\", type=str, required=False, default=argparse.SUPPRESS)\n\
      _parser.add_argument(\"--nb-classes\", dest=\"nb_classes\", type=str, required=False,\
      \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--display-name\", dest=\"\
      display_name\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
      ----output-paths\", dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args\
      \ = vars(_parser.parse_args())\n_output_files = _parsed_args.pop(\"_output_paths\"\
      , [])\n\n_outputs = createTraningJob(**_parsed_args)\n\n_outputs = [_outputs]\n\
      \n_output_serializers = [\n    _serialize_str,\n\n]\n\nimport os\nfor idx, output_file\
      \ in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n\
      \    except OSError:\n        pass\n    with open(output_file, 'w') as f:\n\
      \        f.write(_output_serializers[idx](_outputs[idx]))\n"
    args:
    - --training-data
    - {inputValue: training_data}
    - --validation-data
    - {inputValue: validation_data}
    - --project
    - {inputValue: project}
    - --location
    - {inputValue: location}
    - --bucket
    - {inputValue: bucket}
    - if:
        cond: {isPresent: batch_size}
        then:
        - --batch-size
        - {inputValue: batch_size}
    - if:
        cond: {isPresent: validation_batch_size}
        then:
        - --validation-batch-size
        - {inputValue: validation_batch_size}
    - if:
        cond: {isPresent: training_ds_size}
        then:
        - --training-ds-size
        - {inputValue: training_ds_size}
    - if:
        cond: {isPresent: validation_ds_size}
        then:
        - --validation-ds-size
        - {inputValue: validation_ds_size}
    - if:
        cond: {isPresent: img_height}
        then:
        - --img-height
        - {inputValue: img_height}
    - if:
        cond: {isPresent: img_width}
        then:
        - --img-width
        - {inputValue: img_width}
    - if:
        cond: {isPresent: nb_classes}
        then:
        - --nb-classes
        - {inputValue: nb_classes}
    - if:
        cond: {isPresent: display_name}
        then:
        - --display-name
        - {inputValue: display_name}
    - '----output-paths'
    - {outputPath: Output}
